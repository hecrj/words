% !TEX encoding = UTF-8 Unicode
% !TEX root = ../report.tex
% 
\section{Investigación}

La información referente al algoritmo \texttt{HyperLogLog} se ha obtenido principalmente
del artículo \citetitle{hll:HyperLogLog} ~\cite{hll:HyperLogLog}.
El artículo presenta una descripción detallada del algoritmo y sus ventajas en relación a otros
algoritmos de estimación de cardinalidad.

En esta sección se introducen y se explican, de manera general, las bases del algoritmo \texttt{HyperLogLog}.

\subsection{Funciones de hash}
\label{hash}

Una \textbf{función de hash} se define como una función que mapea todo elemento de un
conjunto de tamaño arbitrario $A$ con otro elemento de un conjunto de tamaño finito $B$.
$$f: A \rightarrow B$$

Una \textbf{buena} función de hash debe ser lo más \textbf{uniforme} posible, es decir, debe mapear los elementos de
$A$ tan \textbf{equitativamente} como sea posible sobre $B$. En otras palabras, todo elemento en $B$ debe ser generado
por la función de hash con \textbf{aproximadamente la misma probabilidad}.

Por lo tanto, si $h$ es una buena función de hash:
$$h: \Sigma^* \rightarrow \{0,1\}^r$$

Donde $\Sigma^*$ representa el conjunto de todas las palabras y $\{0, 1\}^r$ es el conjunto de cadenas binarias de $r$ bits.
Entonces, dada una palabra cualquiera $w \in \Sigma^*$, se puede esperar que:

\begin{align*}
  \left.\begin{array}{r@{\mskip\thickmuskip}l}
    p, a \in \{0, 1\} \\
    p \neq a \\
    1 \leq k \leq r
  \end{array} \right\}
  \quad \implies \quad
  \left.
    P\left( h(w) = p^{k-1} \: a \: b_{k+1} \: b_{k+2} \; ... \; b_r \right) = 2^{-k}
  \right.
\end{align*}

Esto significa que la probabilidad de que la \textbf{primera ocurrencia} de $1$ o $0$ en $h(w)$ sea en el bit $k$ es $2^{-k}$.
Es decir, hay un 50\% de probabilidades de que $h(w)$ empiece por 1, un 25\% de que empiece por $01$, un 12.5\% de que
empiece por $001$, y así sucesivamente.

\subsection{Descripción general del algoritmo}
\label{investigacion:descripcion}

La idea principal de \texttt{HyperLogLog} gira en torno a la propiedad de \textbf{uniformidad} que presentan las buenas
funciones de hash.

Dada una buena función de hash $h$ (igual que en la sección \ref{hash}):
$$h: \Sigma^* \rightarrow \{0,1\}^r$$

Y si el multiconjunto de entrada $M$ tiene $N$ elementos y $n$ elementos únicos, entonces puede esperarse que:

\begin{align*}
  \left.\begin{array}{r@{\mskip\thickmuskip}l}
    p, a \in \{0, 1\} \\
    p \neq a \\
    N \geq 2 \\
    k = \floor{log_2(N)}
  \end{array} \right\}
  \quad \implies \quad
  \left.
    \exists{w \in M}\!: h(w) = p^{k-1} \: a \: b_{k+1} \: b_{k+2} \: ... \: b_r
  \right.
\end{align*}

Por ejemplo, si $M$ tiene \textbf{4 elementos} ($N = 4$) entonces se puede esperar que \textbf{exista uno tal que su hash
empiece por 01}. Si $N = 8$ entonces uno debería empezar por 001, y así sucesivamente.

Lo interesante, sin embargo, es \textbf{hacerlo a la inversa}.
Sea $a \in \{0, 1\}$ y $k_{max}$ la posición \textbf{más tardía} en la que se ha observado
una ocurrencia de $a$ para todo hash de todo elemento de $M$ entonces se puede estimar que $n$ es $E = 2^{k_{max}}$.
Por ejemplo, si tras aplicar $h$ a todo elemento de $M$ se observa que la posición más tardía en la que se ha observado un
$1$ es 3 (empieza por $\textbf{001}$), entonces se puede estimar que $n$ es $E = 2^3 = \textbf{8}$ \textbf{elementos}.

Sin embargo, esta estimación es realmente \textbf{imprecisa}. No es difícil ver que con este estimador
\textbf{la cardinalidad se reduce a simples potencias de 2}. Para eliminar esta imprecisión se usa una tabla $T$ de $m$ entradas
para guardar diferentes $k_{max}$, obteniendo el índice $i$ a partir de los primeros $\floor{log_2(m)}$ bits del hash devuelto
y $k_{candidato}$ con los restantes. A partir de $T$ es posible mejorar el estimador anterior:

$$ E = 2 ^ { \frac{1}{m} \sum\limits_{i=0}^{m}\! T[i] } \cdot m $$

Sin embargo, análisis estadístico realizado por \emph{Flajolet} ~\cite{hll:HyperLogLog} muestra que este estimador
tiene cierta \textbf{tendencia a hacer estimaciones grandes}. Para corregirlo, la estimación se suele multiplicar por la
\textbf{constante 0.79402}, encontrada de manera \textbf{experimental} por el mismo \emph{Flajolet}.
Este estimador es el que utiliza el algoritmo \texttt{LogLog}, una versión anterior a \texttt{HyperLogLog}, y tiene
\textbf{un error estándar} de  $\frac{1.30}{\sqrt{m}}$.

\texttt{HyperLogLog} se diferencia del algoritmo \texttt{LogLog} en que aplica la \textbf{media armónica} en vez de la
media aritmética, \textbf{reduciendo así el error estándar} hasta $\frac{1.04}{\sqrt{m}}$.

Por último, si la estimación resultante es muy alta o muy baja entonces se lleva a cabo una corrección:

\begin{description}
\item[Estimación baja] Si $E < \frac{5m}{2}$, se pueden haber dado casos en que haya \textbf{posiciones vacías en la tabla} que
influyan en el valor de la estimación. En éste caso, se cuentan cuantas de estas posiciones vacías
hay, y en caso de que haya por lo menos una, se usa un nuevo valor $E*$ para la estimación: 
$$E* = m \cdot log\left(\frac{m}{V}\right)$$

Siendo $V$ el número de posiciones vacías. Esta fórmula viene dada por las propiedades de las
\textbf{asignaciones aleatorias}. Éstas indican que si $n$ pelotas son lanzadas aleatoriamente a $m$ canastas entonces
se puede esperar que el número de canastas vacías sea $m \cdot e^{-\mu}$, donde $\mu = n / m$. Por tanto, si se observan
$V$ posiciones vacías sobre un total de $m$ es de esperar que $\mu$ sea cercano a $\log(m/V)$, por lo que $n$ estará
cerca de $m \cdot log(m/v)$.

\item[Estimación alta] Asumiendo una función de hash de 32 bits. Si $E > \frac{2^{32}}{30}$, es de esperar que se hayan
producido \textbf{muchas colisiones} que hayan afectado a la estimación final. La corrección consiste en usar la siguiente $E*$
como sustituto de $E$:
$$E* = -2^{32} \cdot log\left(1 - \frac{E}{2^{32}}\right)$$

Para esta fórmula, se usa el mismo modelo de las canastas del punto anterior, pero se sustituye $m$
por $2^r$, siendo $r$ el número de bits usados en la función de hash. Es decir, $E$ \textbf{estima el número de valores diferentes
a los que se les ha aplicado la función de hash}, que será, con una alta probabilidad, próximo a $2^r \cdot (1 - e^{\frac{-n}{2^r}})$.
Y aislando: $n=-2^r \cdot log\left(1 - \frac{E}{2^r}\right)$.
\end{description}

El algoritmo \ref{algoritmo:hyper1} muestra la versión de \texttt{HyperLogLog} descrita en esta sección.

\begin{algorithm}[h!]
\caption{\texttt{HyperLogLog} para funciones de hash de 32 bits}
\label{algoritmo:hyper1}
\textit{Let $h: D\rightarrow \{0,1\}^{32}$ hash data from D to binary 32-bit word.}

\textit{Let $\rho(s)$ be the position of the leftmost 1-bit of s: e.g.,
$\rho(1...) = 1, \rho(0001...) = 4, \rho(0^K) = K + 1$.}

\textbf{define} $\alpha_{16}=0.673;\alpha_{32}=0.697;\alpha_{64}=0.709;\alpha_m=0.7213/(1+1.079/m)$
for $m \geq 128;$

\textbf{Program \texttt{HYPERLOGLOG}} (\textbf{input} $M$: multiset of items from domain $D$).

\textbf{assume} $m=2^b$ with $ b\in[4..16]$.

\textbf{initialize} a collection of $m$ registers, $M[1],...,M[m]$, to 0;

\begin{algorithmic}
    \FOR{$v\in M$}
            \STATE $x  := h(v)$
            \STATE $j   := 1 + (x_1 x_2 ... x_b)_2$ \COMMENT{binary address determined by the first b bits of x}
            \STATE $w := x_{b+1} x_{b+2} ... $
            \STATE $M[j] := max(M[j],\rho(w))$
    \ENDFOR

    \STATE $E:=\alpha _m m^2·\left(\sum\limits_{j=1}^m 2^{-M[j]}\right)^{-1}$ \COMMENT{the raw HyperLogLog estimate}
    \IF{$E \leq \frac{5}{2}m$}
        \STATE $V :=$ the number of registers equal to $0$
        \STATE \algorithmicif\ $V \neq 0$ \algorithmicthen\ $E* := m \cdot log(m / V)$ \algorithmicelse\ $E* := E$
        \COMMENT{small range correction}
    \ENDIF
    \IF{$E\leq \frac{1}{30}2^{32}$}
        \STATE $E*:=E$ \COMMENT{intermediate range -- no correction}
    \ELSE
        \STATE $E* := -2^{32}log(1-E/2^{32})$ \COMMENT{large range correction}
    \ENDIF
    \RETURN{cardinality estimate E* with typical relative error $\pm$ 1.04/$\sqrt{m}$}
\end{algorithmic}
\end{algorithm}
