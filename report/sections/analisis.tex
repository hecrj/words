% !TEX encoding = UTF-8 Unicode
% !TEX root = ../report.tex
% 
\section{Análisis estadístico del algoritmo}
\label{analisis}

\subsection{Memoria fijada a 1024 bytes}

En esta sección se estudia el comportamiento del algoritmo \texttt{HyperLogLog} en \textbf{9 \emph{datasets}} distintos limitando
la cantidad de memoria utilizada a \textbf{1024 bytes}. En el apéndice \ref{graficas} se incluyen gráficas que muestran los resultados
obtenidos para cada \emph{dataset} de forma detallada. Lo importante, sin embargo, es
\textbf{observar los resultados obtenidos de manera general para cada muestra}, compuestas por 200 ejecucions del programa.

Se presentan las tablas \ref{tabla:count_1024} y \ref{tabla:resumen_1024} a modo de resumen para cada \emph{dataset}.
\clearpage

\begin{table}[h!]
    \centering
    \begin{tabular}{l r r r S S S}
    \strong{Dataset} & \strong{n} & \strong{N} & \strong{Est. media} &
    \strong{SE} & \textbf{T. medio ($ms$)} & \textbf{T. elem. ($\mu s$)}\\ \hline
    \newcounter{dataset}
\forloop{dataset}{1}{\value{dataset} < 10}{
\textbf{D\arabic{dataset}} &
\input{../data/D\arabic{dataset}/summary_1024.tex}
}
\end{tabular}
    \caption{Memoria fijada a 1024 bytes. Resumen de los resultados.}
    \label{tabla:resumen_1024}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{l r r r r r r}
    \strong{Dataset} & \strong{[0, 1)} & \strong{[1, 5)} & \strong{[5, 10)} &
    \strong{[10, 15)} & \textbf{[15, 20)} & \textbf{[20, 100]} \\ \hline
\forloop{dataset}{1}{\value{dataset} < 10}{
\textbf{D\arabic{dataset}} &
\input{../data/D\arabic{dataset}/count_1024.tex}
}
\end{tabular}
    \caption{Memoria fijada a 1024 bytes. Clasificación de las ejecuciones según el error relativo (\%).}
    \label{tabla:count_1024}
\end{table}

Si se toma el \textbf{SE} de la tabla \ref{tabla:resumen_1024}, se puede observar que oscila alrededor del $3\%$, con picos de hasta el $3.6\%$ y un mínimo en $2.7\%$.

Estos valores no parecen seguir ningún tipo de relación con \textbf{N}, más allá del hecho que el \emph{dataset} con la menor \textbf{N}, \textit{D1}, es el que tiene un \textbf{SE} menor. No obstante, el mayor \emph{dataset}, \textit{D8}, tiene un valor por debajo de la media, por lo que \textbf{N} no parece influir en el \textbf{SE}.

Por su parte, los \textbf{tiempos medios} sí que tienen un relación, bastante obvia, con \textbf{N}: cuanto mayor sea \textbf{N} más tardará el programa. Siendo una relación tan lógica no se profundizará más.

Los \textbf{tiempos por elemento}, al contrario, no siguen ningún tipo de relación aparente con \textbf{N}, por lo que deben estar influenciados por otros factores no tan evidentes.

Finalmente, en la tabla \ref{tabla:count_1024} se observa que los valores de \textbf{los errores relativos} están
concentrados, en todos los \emph{datasets}, en el intervalo $[0,10)$, con tan solo 2 valores en total por encima del $10\%$.
Este dato es muy positivo, ya que permite comprobar que el programa no precisa de muchas ejecuciones para dar una
estimación precisa, ya que cada ejecución tiene un \textbf{error relativo} razonablemente bajo. Por otro lado, este dato también
nos muestra que la precisión del algoritmo no depende del multiconjunto de datos de entrada o \emph{dataset}.

\subsection{Influencia de la memoria disponible}

\begin{table}[h!]
    \centering
    \begin{tabular}{l r r r S S S}
    \strong{Memoria (bytes)} & \strong{n} & \strong{N} & \strong{Est. media} &
    \strong{SE} & \textbf{T. medio ($ms$)} & \textbf{T. elem. ($\mu s$)}\\ \hline

\textbf{32} & \input{../data/D1/summary_32.tex}
\textbf{64} & \input{../data/D1/summary_64.tex}
\textbf{128} & \input{../data/D1/summary_128.tex}
\textbf{256} & \input{../data/D1/summary_256.tex}
\textbf{512} & \input{../data/D1/summary_512.tex}
\textbf{1024} & \input{../data/D1/summary_1024.tex}
\textbf{2048} & \input{../data/D1/summary_2048.tex}
\textbf{4096} & \input{../data/D1/summary_4096.tex}
\textbf{8192} & \input{../data/D1/summary_8192.tex}
\textbf{16384} & \input{../data/D1/summary_16384.tex}

\end{tabular}
    \caption{Influencia de la memoria sobre el dataset D1. Resumen de resultados.}
    \label{tabla:resumen_1024}
\end{table}

\begin{table}[h!]
    \centering
    \begin{tabular}{l r r r r r r}
    \strong{Memoria} & \strong{[0, 1)} & \strong{[1, 5)} & \strong{[5, 10)} &
    \strong{[10, 15)} & \textbf{[15, 20)} & \textbf{[20, 100]} \\ \hline

\textbf{32} & \input{../data/D1/count_32.tex}
\textbf{64} & \input{../data/D1/count_64.tex}
\textbf{128} & \input{../data/D1/count_128.tex}
\textbf{256} & \input{../data/D1/count_256.tex}
\textbf{512} & \input{../data/D1/count_512.tex}
\textbf{1024} & \input{../data/D1/count_1024.tex}
\textbf{2048} & \input{../data/D1/count_2048.tex}
\textbf{4096} & \input{../data/D1/count_4096.tex}
\textbf{8192} & \input{../data/D1/count_8192.tex}
\textbf{16384} & \input{../data/D1/count_16384.tex}

\end{tabular}
    \caption{Influencia de la memoria sobre el dataset D1. Clasificación de las ejecuciones según el error relativo (\%).}
    \label{tabla:count_1024}
\end{table}

Para estudiar el impacto de la \textbf{memoria disponible} en los resultados se ha usado el \emph{dataset} \textit{D1},
con valores de memoria que oscilan entre los $32 bytes$ y los $16 Kbytes$.

En la tabla \ref{tabla:resumen_1024} se observa que en todos los casos la estimación media es razonablemente precisa, siendo incluso
tan precisa la estimación con $64 bytes$ como la que disponía de $4 Kb$. Esto podria llevar a la conclusión, errónea, de que el tamaño
de la memoria es irrelevante para el resultado final. No obstante, se observa que cuanto mayor es la
memoria usada menor es el \textbf{SE}. Si se analiza la tabla \ref{count_1024} se puede apreciar que los \textbf{errores relativos} de
 las ejecuciones con memorias más reducidas se concentran en los valores superiores al $10\%$. No es hasta que la
\textbf{memoria disponible} aumenta hasta los 512 bytes que el \textbf{error relativo} se concentra por debajo del $10\%$, y solo a
partir de los $4 Kbytes$ dejan de aparecer ejecuciones por encima del $5\%$. Por tanto, para obtener los mismos resultados que
usando más memoria se deberían usar muchas más muestras, cosa que enlantecería el proceso, tal vez más de lo posible.

En general se podría concluir que el mínimo de \textbf{memoria necesaria} es de unos $512 bytes$, ya que su \textbf{SE} es del $5\%$
aproximadamente, un valor aceptable en la mayoría de casos. Por otro lado, dependiendo de la importancia de la precisión, se puede
aumentar o disminuir este valor.
