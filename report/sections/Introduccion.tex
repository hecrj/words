\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\begin{document}

\title{Investigacion biblografica y analisis del algoritmo}
\maketitle

La información para implementar el algoritmo se ha sacado principalmente del artículo \texttt{HyperLogLog: the analysis of a near-optimal cardinality estimation algortihm}, de Philippe Flajolet, Éric Fusy, Olivier Gandouet y Fréderic Meunier. En éste se detalla el comportamiento del algoritmo HyperLogLog, así como sus ventajas con relación a otros algoritmos de estimación de cardinalidad. Así mismo, ofrece dos implementaciones en pseudocódigo del algoritmo, uno de los cuales es una versión mejorada de la otra. La versión que se ha implementado (\texttt{Figura 1}) es la mejorada, cuyas principales mejoras respecto al otro son que corrige los errores de estimación que se pueden producir cuando $n$ es demasiado grande o bien demasiado pequeña.

\fbox{
\begin{minipage}{\dimexpr\textwidth-2\fboxsep-2\fboxrule\relax}
\textit{Let $h: D\rightarrow{0,1}^{32}$ hash data from D to binary 32-bit word.}

\textit{Let $\rho(s)$ be the position of the leftmost 1-bit of s: e.g., $\rho(1...) = 1, \rho(0001...) = 4, \rho(0^K) = K + 1$.}

\textbf{define} $\alpha_{16}=0.673;\alpha_{32}=0.697;\alpha_{64}=0.709;\alpha_m=0.7213/(1+1.079/m)$ for $m \geq 128;$

\textbf{Program \texttt{HYPERLOGLOG} (input }M: multiset of items from domain D).

\textbf{assume} $m=2^b$ with $ b\in[4..16]$.

\textbf{initialize} a collection of $m$ registers, $M[1],...,M[m]$, to 0;

\textbf{for} $v\in M$ \textbf{do}

	\textbf{set} $x:=h(v);$
	
	\textbf{set} $j=1+(x_1x_2...x_b)_2;$	\textit{{binary address determined by the first b bits of x}}
	
	 \textbf{set} $w:=x_{b+1}x_{b+2}...;$
	 
	 \textbf{set} $M[j]:=$max$(M[j],\rho(w)):$
	 
\textbf{compute} $E:=\alpha _m m^2·\left(\sum\limits_{j=1}^m 2^{-M[j]}\right)^{-1}$	\textit{{the "raw" HyperLogLog estimate}}

\textbf{if} $E\leq \frac{5}{2}m$ \textbf{then}

	\textbf{let} $V$ be the number of registers equal to 0;

	\textbf{if} $V\neq 0$ \textbf{then set} $E*:=m·log(m/V)$ \textbf{else set} $E*:=E;$	
\textit{{small range correction}}

\textbf{if} $E\leq \frac{1}{30}2^{32}$ \textbf{then}

	\textbf{set} $E*:=E;$	\textit{{intermediate range$\rightarrow$no correction}}

\textbf{if} $E > \frac{1}{30}2^{32}$ \textbf{then}
	
	\textbf{set} $E*:=-2^{32}log(1-E/2^{32});$	\textit{{large range correction}}
	
\textbf{return} \textit{cardinality estimate E* with typical relative error $\pm$ 1.04/$\sqrt{m}$.}
\end{minipage}
}


\textit{Figura 1}
\\

Éste algoritmo usa la aleatoriedad que proporcionan las funciones de hash para calcular una estimación de la cardinalidad de un conjunto de valores. La idea básica del algoritmo es la siguiente: 
Entre las propiedades de las funciones de hash está el que los bits de la salida son independientes y cada uno tiene un 50$\%$ de posibilidades de ocurrir. Teniendo esto en cuenta, se puede esperar que:
\\

	El 50$\%$ del output sea de la forma 1X...X
	
	El 25$\%$ del output sea de la forma 01X...X
	
	El 12.5$\%$ del output sea de la forma 001X...X
	
	Etc...
\\
	
Por lo que si, por ejemplo, se tienen 8 valores, se puede esperar que 1 sea de la forma 001X...X, si se tienen 4, 1 será de la forma 01X...X, etc. Y de aquí se salta a la inversa: si la primera posición (si se empieza a contar desde el primer bit) en la que aparece un 1 és el índice 2 (tomando el primer bit como índice 0), se puede esperar que haya 8 elementos, si el índice és 3 se puede esperar que haya 16 elementos y así sucesivamente.

No obstante, éste resultado será, en el mejor de los casos, aproximado, y con un margen de error muy ámplio, por lo que se tiene que optimizar para que sea viable. Para ello se usa una tabla en la que se guardan varias estimaciones, y en la que se usan los primeros bits del valor de hash para determinar el índice, y el resto de bits para calcular la estimación. Finalmente, una vez se tienen todas las estimaciones, se usa, y aquí es donde HyperLogLog se diferencia del algoritmo LogLog, la media armónica de todas ellas para obtener el valor aproximado final.
Finalmente, como ya se ha comentado, si se tiene una estimación (llamada $E$) con valores muy grandes o bien muy bajos, se lleva a cabo una corrección. Hay 3 casos:
\\

1. Si $E < 5m/2$, se pueden haber dado casos en que haya posiciones vacías en la tabla que perviertan el valor de la estimación. En éste caso, se cuentan cuantas de estas posiciones vacías hay, y en caso de que haya por lo menos uno, se usa un nuevo valor (E*) para la estimación: 
$$E* = m*log(m/V)$$
Siendo $V$ el número de posiciones vacías. Esta fórmula viene dada por las propiedades de las asignaciones aleatorias. Éstas indican que, dadas $m$ canastas, y $n$ lanzamientos de pelotas, se puede esperar que el número de canastas vacías sería me$\mu$, con $\mu=n/m$. Por tanto, si observamos $V$ posiciones vacías sobre un total de $m$, se puede esperar que $\mu$ sea cercano a $\log(m/V)$, por lo que $n$ estará cerca de $m*\log(m/v)$.
\\

2. Si $E > 2^{32}/30$, se producirán demasiadas colisiones en la función de hash que llegarían a afectar al resultado final. En éste caso se usa la siguiente $E*$ como sustituto de $E$:
$$E* = -2^{32}log(1-E/2^{32})$$
Para esta fórmula, se usa el mismo modelo de las canastas del punto anterior, pero se sustituye $m$ por $2^L$, siendo $L$ el número de bits usados en la función de hash, normalmente 32 para $n \subset 1...10^9$. Es decir, $E$ estima el número de valores de hash diferente, que será, con una alta probabilidad, próximo a $2^L(1-e^{-k})$, siendo $k = n/2^L$. Por tanto, si aislamos n nos da que 
$n=-2^Llog(1-E/2^L)$
\\

3. En el caso de que $E$ esté entre éstos dos valores, la estimación entra dentro de los valores “normales”, por lo que no hace falta modificarla.
	
\end{document}